1. Successfully create a backup/restoration of the database 
2. Create .sql scripts representing the tables. Store these in version control. 
3. Create Views for successful complicated queries 
4. Add tables / functionality to deal with different currencies
5. The .sql scripts such as changing schema, placing primary and foreign keys and updating the orders_table are done after the ETL process. 
   Is there a way to automate all of this? 
6. Addition of logging to document progress of ETL pipeline. 
7. Various code improvements (Will have to document after a code review)
8. Addition of unittests and validation tests for the ETL process 
9. Implementation of GitHub Actions (Will research)

Other Questions: 

- What if a certain product is removed from the company?
  What could be done inside the data warehouse to accomodate this? 
  (Update the product_key to be set to either 0 or -1 ? )

- How do we deal with old data? 

Two directions this project can go into: 

1. The BI Side 

Importing the data model into PowerBI and running some insights against it, 
instead of using SQL. 

2. The Data Engineering / Database Admin side 

Focuses more on improving the current code to make it more flexible, 
and improving the robustness of the data model to handle either: 

- More data sources 
- Updates to current data within the warehouse. 

3. The Cloud engineering / DevOps Side 

- Setting up the database on a cloud provider 
- Running the python scripts in an automated fashion 
- Running the project within a Docker Container? 